{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9bb0e69",
   "metadata": {},
   "source": [
    "The computer cannot directly understand a text because it does not have a structure (Unstructured), for processing some tags need to be added to the text.\n",
    "\n",
    "*Text(Unstructured)* --> *TOKENIZATION* --> *Stemming* / *Lemmatization* --> *Parts of Speech Tagging* --> *N.E.R(Name Entity Recognition)* --> *Processed text(Structured)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c76a67",
   "metadata": {},
   "source": [
    "> Encoding letters v/s Encoding Words (Wats better?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2aaff",
   "metadata": {},
   "source": [
    "**Tokenization and Sequencing**\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5165d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb95e5",
   "metadata": {},
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c21b923",
   "metadata": {},
   "source": [
    "The tokenizer doesnt is not case sensitive and it also ignores any punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7261228c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct tokens: 98\n",
      "\n",
      "{'the': 1, 'and': 2, 'of': 3, 'a': 4, 'harry': 5, 'potter': 6, 'series': 7, 'wizard': 8, 'in': 9, 'fantasy': 10, 'novels': 11, 'by': 12, 'british': 13, 'all': 14, 'school': 15, 'story': 16, 'united': 17, 'many': 18, 'includes': 19, 'is': 20, 'seven': 21, 'written': 22, 'author': 23, 'j': 24, 'k': 25, 'rowling': 26, 'chronicle': 27, 'lives': 28, 'young': 29, 'his': 30, 'friends': 31, 'ron': 32, 'weasley': 33, 'hermione': 34, 'granger': 35, 'whom': 36, 'are': 37, 'students': 38, 'at': 39, 'hogwarts': 40, 'witchcraft': 41, 'wizardry': 42, 'main': 43, 'arc': 44, 'concerns': 45, \"harry's\": 46, 'conflict': 47, 'with': 48, 'lord': 49, 'voldemort': 50, 'dark': 51, 'who': 52, 'intends': 53, 'to': 54, 'become': 55, 'immortal': 56, 'overthrow': 57, 'governing': 58, 'body': 59, 'known': 60, 'as': 61, 'ministry': 62, 'magic': 63, 'subjugate': 64, 'wizards': 65, 'muggles': 66, 'non': 67, 'magical': 68, 'people': 69, 'was': 70, 'originally': 71, 'published': 72, 'english': 73, 'bloomsbury': 74, 'kingdom': 75, 'scholastic': 76, 'press': 77, 'states': 78, 'genres': 79, 'including': 80, 'drama': 81, 'coming': 82, 'age': 83, 'fiction': 84, 'which': 85, 'elements': 86, 'mystery': 87, 'thriller': 88, 'adventure': 89, 'horror': 90, 'romance': 91, 'world': 92, 'explores': 93, 'numerous': 94, 'themes': 95, 'cultural': 96, 'meanings': 97, 'references': 98} \n",
      "\n",
      "[[5, 6, 20, 4, 7, 3, 21, 10, 11, 22, 12, 13, 23, 24, 25, 26], [1, 11, 27, 1, 28, 3, 4, 29, 8, 5, 6, 2, 30, 31, 32, 33, 2, 34, 35, 14, 3, 36, 37, 38, 39, 40, 15, 3, 41, 2, 42], [1, 43, 16, 44, 45, 46, 47, 48, 49, 50, 4, 51, 8, 52, 53, 54, 55, 56, 57, 1, 8, 58, 59, 60, 61, 1, 62, 3, 63, 2, 64, 14, 65, 2, 66, 67, 68, 69], [1, 7, 70, 71, 72, 9, 73, 12, 74, 9, 1, 17, 75, 2, 76, 77, 9, 1, 17, 78, 4, 7, 3, 18, 79, 80, 10, 81, 82, 3, 83, 84, 2, 1, 13, 15, 16, 85, 19, 86, 3, 87, 88, 89, 90, 2, 91, 1, 92, 3, 5, 6, 93, 94, 95, 2, 19, 18, 96, 97, 2, 98], []]\n"
     ]
    }
   ],
   "source": [
    "f=open(\"book.txt\",\"r\")\n",
    "d=f.read()\n",
    "sentences=d.split(\"\\n\")\n",
    "\n",
    "tokenizer= Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(sentences)      # This tokenizer instance, is fit onto the given sentences.\n",
    "\n",
    "word_dict= tokenizer.word_index\n",
    "print(f\"number of distinct tokens: {list(word_dict.values())[-1]}\\n\")\n",
    "print(word_dict,\"\\n\")\n",
    "\n",
    "sequences= tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68bdb93",
   "metadata": {},
   "source": [
    "**Sequencing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70c2c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct tokens: 98\n",
      "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n",
      "[[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 5], [1, 2, 3, 4]]\n"
     ]
    }
   ],
   "source": [
    "sentences= [\"I LOVE my dog\",\n",
    "            \"i love my dog\",\n",
    "            \"I love my cat\",\n",
    "            \"I love my dog!!\"]\n",
    "\n",
    "tokenizer= Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(sentences)      # This tokenizer instance, is fit onto the given sentences.\n",
    "\n",
    "print(f\"number of distinct tokens: {list(word_dict.values())[-1]}\")\n",
    "\n",
    "word_dict= tokenizer.word_index  # Extract tokens\n",
    "print(word_dict)\n",
    "\n",
    "sequences= tokenizer.texts_to_sequences(sentences)   # Convert to sequences\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708e193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n",
      "[[1, 2, 3, 4], [3, 4]]\n"
     ]
    }
   ],
   "source": [
    "test_data= ['I realy love my dog', 'my dog loves eating']\n",
    "\n",
    "# I am using the above generated tokenizer to extract sequences from test_data\n",
    "test_sqn= tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "# Observe how a 5 word sentences turn into 4 word sentences, so some data is lost\n",
    "print(word_dict)\n",
    "print(test_sqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459a0474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of distinct tokens: 6\n",
      "{'<OOV>': 1, 'i': 2, 'love': 3, 'my': 4, 'dog': 5, 'cat': 6}\n",
      "[[2, 1, 3, 4, 5], [4, 5, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "sentences= [\"I LOVE my dog\",\n",
    "            \"i love my dog\",\n",
    "            \"I love my cat\",\n",
    "            \"I love my dog!!\"]\n",
    "\n",
    "# OOV stands for \"Out-Of-Volcablury\", this property will generate token for unseen token and hences preserve the sentence.\n",
    "tokenizer= Tokenizer(num_words=100, oov_token=\"<OOV>\")  \n",
    "tokenizer.fit_on_texts(sentences)    \n",
    "\n",
    "\n",
    "# Here, we can see that the very first token is <OOV> and this will be used for all the instances of unseen tokens in test_data.\n",
    "word_dict= tokenizer.word_index  \n",
    "sequences= tokenizer.texts_to_sequences(sentences)   \n",
    "print(word_dict)\n",
    "\n",
    "test_data= ['I realy love my dog', 'my dog loves eating']\n",
    "test_sqn= tokenizer.texts_to_sequences(test_data)\n",
    "print(test_sqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd636662",
   "metadata": {},
   "source": [
    "In order to feed this Data into a NN, the size of the sentences should be the same we will pad the sentences, so we pad them with zero, similar to how we used to do with Images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96cbe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'i': 2, 'love': 3, 'my': 4, 'dog': 5, 'cat': 6, 'and': 7, 'he': 8, 'loves': 9, 'me': 10, 'too': 11} \n",
      "\n",
      "[[2, 3, 4, 5], [2, 3, 4, 5], [2, 3, 4, 6], [2, 3, 4, 5, 7, 8, 9, 10, 11]] \n",
      "\n",
      "[[ 0  0  0  0  0  2  3  4  5]\n",
      " [ 0  0  0  0  0  2  3  4  5]\n",
      " [ 0  0  0  0  0  2  3  4  6]\n",
      " [ 2  3  4  5  7  8  9 10 11]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences= [\"I LOVE my dog\",\n",
    "            \"i love my dog!!\",\n",
    "            \"I love my cat\",\n",
    "            \"I love my dog and he loves me too\"]\n",
    "\n",
    "tokenizer= Tokenizer(num_words=100, oov_token=\"<OOV>\")  \n",
    "tokenizer.fit_on_texts(sentences)    \n",
    "\n",
    "word_dict= tokenizer.word_index  \n",
    "sequences= tokenizer.texts_to_sequences(sentences)  \n",
    "print(word_dict,\"\\n\") \n",
    "print(sequences, \"\\n\")\n",
    "\n",
    "# pad_sequences(< sequences >, < padding=(pre/post) >, < truncating=(pre/post), < maxlen=(number) >)\n",
    "# truncating parameter is used if we use maxlen, where to truncate the sentences it len(sentence)>maxlen.\n",
    "padded_sqn= pad_sequences(sequences, padding='pre')\n",
    "print(padded_sqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2520fa5a",
   "metadata": {},
   "source": [
    "**Sentiment Anlysis of Sarcasm**\n",
    "-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b73d90a",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c791ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f=open(\"Sarcasm_Headlines_Dataset_v2.json\",\"r\")\n",
    "\n",
    "sentences= []\n",
    "labels= []\n",
    "urls= []\n",
    "\n",
    "for line in f:\n",
    "    item=json.loads(line)\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e042fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28619\n",
      "thirtysomething scientists unveil doomsday clock of hair loss\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(sentences[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93617b9c",
   "metadata": {},
   "source": [
    "Generating Training and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47025ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22895\n",
      "22895\n",
      "5724\n",
      "5724\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "training_sentences, testing_sentences, train_label, test_label= tts(sentences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(training_sentences))\n",
    "print(len(train_label))\n",
    "print(len(testing_sentences))\n",
    "print(len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba70a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim= 10000\n",
    "embedding_dim= 16\n",
    "max_len= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f12cba",
   "metadata": {},
   "source": [
    "Initilizing training tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256ae23",
   "metadata": {},
   "source": [
    "1. After loading the dataset, initiate the tokenizer and fit it only on the Training set( training_sentences).\n",
    "2. Create the sequences using the tokenizer and the training_sentence.\n",
    "3. Th3e labels and sequences must be converted to npmpy array for the sequences tro be processed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4942942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of tokens: 27770\n",
      "[  27   13  109  638   17  781   67 4774    5   43 1939    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "(22895, 100)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer= Tokenizer(num_words=input_dim, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_dict= tokenizer.word_index\n",
    "print(f\"No. of tokens: {list(word_dict.values())[-1]}\")\n",
    "\n",
    "sequences= tokenizer.texts_to_sequences(training_sentences)\n",
    "padded_sqn= pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "print(padded_sqn[0])\n",
    "print(padded_sqn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f7b7d",
   "metadata": {},
   "source": [
    "Tokenizing Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9880dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences= tokenizer.texts_to_sequences(testing_sentences)\n",
    "padded_sqn_test= pad_sequences(test_sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42426eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "padded_sqn= np.array(padded_sqn)\n",
    "train_label= np.array(train_label)\n",
    "padded_sqn_test= np.array(padded_sqn_test)\n",
    "test_label= np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ab0d2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AYUSH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model=models.Sequential([\n",
    "    layers.Embedding(input_dim, embedding_dim, input_length= max_len),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(24, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ed44114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92283905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5428 - loss: 0.6880 - val_accuracy: 0.7790 - val_loss: 0.5919\n",
      "Epoch 2/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7585 - loss: 0.5305 - val_accuracy: 0.8129 - val_loss: 0.4222\n",
      "Epoch 3/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8283 - loss: 0.3926 - val_accuracy: 0.8307 - val_loss: 0.3827\n",
      "Epoch 4/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8577 - loss: 0.3351 - val_accuracy: 0.8125 - val_loss: 0.3978\n",
      "Epoch 5/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8678 - loss: 0.3039 - val_accuracy: 0.8457 - val_loss: 0.3453\n",
      "Epoch 6/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8891 - loss: 0.2731 - val_accuracy: 0.8513 - val_loss: 0.3372\n",
      "Epoch 7/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8989 - loss: 0.2470 - val_accuracy: 0.8220 - val_loss: 0.3982\n",
      "Epoch 8/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9030 - loss: 0.2363 - val_accuracy: 0.8468 - val_loss: 0.3591\n",
      "Epoch 9/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9164 - loss: 0.2122 - val_accuracy: 0.8318 - val_loss: 0.3970\n",
      "Epoch 10/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9150 - loss: 0.2114 - val_accuracy: 0.8491 - val_loss: 0.3458\n",
      "Epoch 11/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9214 - loss: 0.1958 - val_accuracy: 0.8573 - val_loss: 0.3460\n",
      "Epoch 12/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9304 - loss: 0.1797 - val_accuracy: 0.8552 - val_loss: 0.3536\n",
      "Epoch 13/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9335 - loss: 0.1727 - val_accuracy: 0.8452 - val_loss: 0.3743\n",
      "Epoch 14/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9379 - loss: 0.1598 - val_accuracy: 0.8363 - val_loss: 0.4198\n",
      "Epoch 15/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9318 - loss: 0.1690 - val_accuracy: 0.8360 - val_loss: 0.4055\n",
      "Epoch 16/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9390 - loss: 0.1562 - val_accuracy: 0.8512 - val_loss: 0.3776\n",
      "Epoch 17/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9476 - loss: 0.1408 - val_accuracy: 0.8323 - val_loss: 0.4338\n",
      "Epoch 18/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9446 - loss: 0.1456 - val_accuracy: 0.8470 - val_loss: 0.3926\n",
      "Epoch 19/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9425 - loss: 0.1492 - val_accuracy: 0.8489 - val_loss: 0.4087\n",
      "Epoch 20/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9457 - loss: 0.1392 - val_accuracy: 0.8498 - val_loss: 0.4081\n",
      "Epoch 21/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9373 - loss: 0.1579 - val_accuracy: 0.8443 - val_loss: 0.4196\n",
      "Epoch 22/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9563 - loss: 0.1228 - val_accuracy: 0.8199 - val_loss: 0.5378\n",
      "Epoch 23/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9518 - loss: 0.1290 - val_accuracy: 0.8284 - val_loss: 0.4826\n",
      "Epoch 24/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9418 - loss: 0.1458 - val_accuracy: 0.8424 - val_loss: 0.4388\n",
      "Epoch 25/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9513 - loss: 0.1239 - val_accuracy: 0.8408 - val_loss: 0.4487\n",
      "Epoch 26/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9534 - loss: 0.1227 - val_accuracy: 0.8370 - val_loss: 0.4795\n",
      "Epoch 27/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9576 - loss: 0.1114 - val_accuracy: 0.8370 - val_loss: 0.4960\n",
      "Epoch 28/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9583 - loss: 0.1139 - val_accuracy: 0.8274 - val_loss: 0.5402\n",
      "Epoch 29/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9530 - loss: 0.1228 - val_accuracy: 0.8323 - val_loss: 0.5132\n",
      "Epoch 30/30\n",
      "\u001b[1m716/716\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9563 - loss: 0.1185 - val_accuracy: 0.8382 - val_loss: 0.4664\n"
     ]
    }
   ],
   "source": [
    "history= model.fit(padded_sqn, train_label, epochs=30, validation_data=(padded_sqn_test, test_label), verbose=1)\n",
    "model.save(\"Sarcasm_detector.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4e85896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "[[0.00449917]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"a rainy day sun shines the best\"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "print(model.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0bba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
