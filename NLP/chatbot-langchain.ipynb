{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# pip install langchain-core langgraph>0.2.27","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:15:27.586967Z","iopub.execute_input":"2025-07-27T17:15:27.587881Z","iopub.status.idle":"2025-07-27T17:15:27.592032Z","shell.execute_reply.started":"2025-07-27T17:15:27.587848Z","shell.execute_reply":"2025-07-27T17:15:27.591093Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.","metadata":{}},{"cell_type":"code","source":"# import getpass\n# import os\n\n# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:15:27.593465Z","iopub.execute_input":"2025-07-27T17:15:27.593814Z","iopub.status.idle":"2025-07-27T17:15:27.612978Z","shell.execute_reply.started":"2025-07-27T17:15:27.593789Z","shell.execute_reply":"2025-07-27T17:15:27.611974Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"LangChain is a framework for building applications powered by large language models (LLMs), and one of its core uses is to help build LLM agents.\n\nHere we will be using Gemini","metadata":{}},{"cell_type":"code","source":"pip install -qU \"langchain[google-genai]\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:15:27.614458Z","iopub.execute_input":"2025-07-27T17:15:27.614752Z","iopub.status.idle":"2025-07-27T17:15:40.899883Z","shell.execute_reply.started":"2025-07-27T17:15:27.614727Z","shell.execute_reply":"2025-07-27T17:15:40.898840Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.8/442.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Get the API of google gemini to use it","metadata":{}},{"cell_type":"code","source":"import getpass\nimport os\n\nif not os.environ.get(\"GOOGLE_API_KEY\"):\n  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:15:40.901118Z","iopub.execute_input":"2025-07-27T17:15:40.901501Z","iopub.status.idle":"2025-07-27T17:16:00.495740Z","shell.execute_reply.started":"2025-07-27T17:15:40.901469Z","shell.execute_reply":"2025-07-27T17:16:00.494736Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter API key for Google Gemini:  ········\n"}],"execution_count":5},{"cell_type":"markdown","source":"Using the API, we can chat with the model, but as of now, it doesn't have any memory, so we can't ask any chain of questions. The reason being it's an LLM, it needs to be integrated with tools to perform such tasks.","metadata":{}},{"cell_type":"code","source":"from langchain_core.messages import HumanMessage\n\nmodel.invoke([HumanMessage(content=\"Hi! I'm Ayush\")])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:16:00.497542Z","iopub.execute_input":"2025-07-27T17:16:00.498071Z","iopub.status.idle":"2025-07-27T17:16:01.095052Z","shell.execute_reply.started":"2025-07-27T17:16:00.498043Z","shell.execute_reply":"2025-07-27T17:16:01.094109Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"AIMessage(content=\"Hi Ayush! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--12603a9d-9355-4862-85d0-a0bbda145dd6-0', usage_metadata={'input_tokens': 7, 'output_tokens': 20, 'total_tokens': 27, 'input_token_details': {'cache_read': 0}})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"model.invoke([HumanMessage(content=\"What's my name?\")])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:16:01.096065Z","iopub.execute_input":"2025-07-27T17:16:01.096401Z","iopub.status.idle":"2025-07-27T17:16:01.721671Z","shell.execute_reply.started":"2025-07-27T17:16:01.096376Z","shell.execute_reply":"2025-07-27T17:16:01.720734Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"AIMessage(content=\"As a large language model, I don't know your name. You haven't told me!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--fa8ffe18-a309-4392-bddd-3d0683351a22-0', usage_metadata={'input_tokens': 6, 'output_tokens': 22, 'total_tokens': 28, 'input_token_details': {'cache_read': 0}})"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"print(model.invoke([HumanMessage(content=\"If i integrate you with tools can you have a memory and tell my name?\")]).content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:16:01.722482Z","iopub.execute_input":"2025-07-27T17:16:01.722734Z","iopub.status.idle":"2025-07-27T17:16:05.094816Z","shell.execute_reply.started":"2025-07-27T17:16:01.722714Z","shell.execute_reply":"2025-07-27T17:16:05.093896Z"}},"outputs":[{"name":"stdout","text":"If I am integrated with tools that provide memory capabilities, then yes, I can remember your name and other information you choose to share with me.\n\nHere's a breakdown of how that works:\n\n*   **Without External Tools:** In my basic form, I don't have a persistent memory. Each conversation is treated as a new session. Once the conversation ends, the information is generally forgotten.\n*   **With External Tools (Memory Systems):** When integrated with tools designed for memory, such as:\n    *   **Databases:** Your name and other information could be stored in a database. I can then query the database to retrieve this information.\n    *   **Vector Stores:** Your conversations, including your name, can be embedded into vectors and stored in a vector store. I can then use semantic search to retrieve relevant information from past conversations.\n    *   **Long-Term Memory Systems:** Some AI frameworks are designed to provide long-term memory for AI agents. These systems often use a combination of techniques to store and retrieve information.\n\nHere's what would happen in a typical scenario:\n\n1.  **You tell me your name:**  You say, \"My name is [Your Name].\"\n2.  **Storage:** The integrated tool detects this and stores your name in its memory system (e.g., database, vector store, etc.).\n3.  **Retrieval:** In a subsequent conversation, you might say, \"Do you remember me?\"\n4.  **Processing:** I would use the memory tool to search for information related to you.\n5.  **Response:** If a match is found, I would respond with something like, \"Yes, I remember you, [Your Name]!\"\n\n**Important Considerations:**\n\n*   **Privacy:**  If you're using tools that store your information, be sure to understand the privacy policies and security measures in place. You should have control over what information is stored and how it's used.\n*   **Tool Specifics:** The exact implementation and capabilities will depend on the specific tools I am integrated with.\n*   **Context is Key:** Even with memory tools, my ability to retrieve information effectively depends on the context of the conversation and how the data is stored.\n\nIn summary, while I don't have built-in memory, integration with the right tools would allow me to remember your name and personalize our interactions based on past conversations.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from langchain_core.messages import AIMessage\n\nlist= [HumanMessage(content=\"Hi! I'm Bob\"),\n       AIMessage(content=\"Hello Cob! How can I assist you today?\"),\n       HumanMessage(content=\"What's my name?\"),\n       HumanMessage(content=\"What is Cob?\")]\nmodel.invoke(\n    list\n)\n\nprint(model.invoke(\n    list\n).content)\nlist.append(model.invoke(\n    list\n))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:16:05.095901Z","iopub.execute_input":"2025-07-27T17:16:05.096172Z","iopub.status.idle":"2025-07-27T17:16:07.425909Z","shell.execute_reply.started":"2025-07-27T17:16:05.096152Z","shell.execute_reply":"2025-07-27T17:16:07.424867Z"}},"outputs":[{"name":"stdout","text":"You introduced yourself as Bob.\n\n\"Cob\" was a typo in my previous response to you. I meant to type \"Bob\". It's a small, easily made mistake.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(list)\nprint(type((list)))\nprint(type(list[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:16:07.427110Z","iopub.execute_input":"2025-07-27T17:16:07.427494Z","iopub.status.idle":"2025-07-27T17:16:07.432691Z","shell.execute_reply.started":"2025-07-27T17:16:07.427461Z","shell.execute_reply":"2025-07-27T17:16:07.431654Z"}},"outputs":[{"name":"stdout","text":"[HumanMessage(content=\"Hi! I'm Bob\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Cob! How can I assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is Cob?', additional_kwargs={}, response_metadata={}), AIMessage(content='You introduced yourself as Bob.\\n\\nAs for \"Cob,\" that was a typo on my part! I meant to say \"Bob\" but made a mistake. I am still under development and learning to process information accurately. Sorry for the confusion!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--fe20cbbc-f9f4-4adf-a56b-b4e1eb5d7b4f-0', usage_metadata={'input_tokens': 26, 'output_tokens': 50, 'total_tokens': 76, 'input_token_details': {'cache_read': 0}})]\n<class 'list'>\n<class 'langchain_core.messages.human.HumanMessage'>\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"list.append(HumanMessage(content=\"Nice to meet you\"))\nfor i in list:\n    print(i)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:16:07.434886Z","iopub.execute_input":"2025-07-27T17:16:07.435192Z","iopub.status.idle":"2025-07-27T17:16:07.456451Z","shell.execute_reply.started":"2025-07-27T17:16:07.435168Z","shell.execute_reply":"2025-07-27T17:16:07.455627Z"}},"outputs":[{"name":"stdout","text":"content=\"Hi! I'm Bob\" additional_kwargs={} response_metadata={}\ncontent='Hello Cob! How can I assist you today?' additional_kwargs={} response_metadata={}\ncontent=\"What's my name?\" additional_kwargs={} response_metadata={}\ncontent='What is Cob?' additional_kwargs={} response_metadata={}\ncontent='You introduced yourself as Bob.\\n\\nAs for \"Cob,\" that was a typo on my part! I meant to say \"Bob\" but made a mistake. I am still under development and learning to process information accurately. Sorry for the confusion!' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--fe20cbbc-f9f4-4adf-a56b-b4e1eb5d7b4f-0' usage_metadata={'input_tokens': 26, 'output_tokens': 50, 'total_tokens': 76, 'input_token_details': {'cache_read': 0}}\ncontent='Nice to meet you' additional_kwargs={} response_metadata={}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from langchain_core.messages import AIMessage\n\nmodel.invoke(\n    list\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T17:16:07.457504Z","iopub.execute_input":"2025-07-27T17:16:07.458002Z","iopub.status.idle":"2025-07-27T17:16:08.082814Z","shell.execute_reply.started":"2025-07-27T17:16:07.457962Z","shell.execute_reply":"2025-07-27T17:16:08.081870Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"AIMessage(content=\"Nice to meet you too, Bob! I'm glad to be interacting with you. Is there anything I can help you with today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--8cfeb0eb-3235-4625-a23b-02ca779729ba-0', usage_metadata={'input_tokens': 79, 'output_tokens': 29, 'total_tokens': 108, 'input_token_details': {'cache_read': 0}})"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"Let's try to build a chatbot that remembers conversation history, with just model.invoke\n\nThis is a simple demonstration of Chatbot using Langchain, it performs a linear chain of tasks that are: Take in the query, store it, respond and store.","metadata":{}},{"cell_type":"code","source":"def chat(list, query):\n    list.append(HumanMessage(content=query))\n    message= model.invoke(list)\n    list.append(message)\n    print(f\"Bot :{message.content}\")\n\nconvo=[]\nprint(\"Press N- to stop\")\nwhile True:\n    query= input(\"You : \")\n    if query == \"N-\":\n        break;\n    chat(convo, query)\n    print(\"=\"*40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T18:46:02.898548Z","iopub.execute_input":"2025-07-27T18:46:02.898916Z","iopub.status.idle":"2025-07-27T18:46:05.911528Z","shell.execute_reply.started":"2025-07-27T18:46:02.898892Z","shell.execute_reply":"2025-07-27T18:46:05.910756Z"}},"outputs":[{"name":"stdout","text":"Press N- to stop\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You :  N-\n"}],"execution_count":26},{"cell_type":"markdown","source":"# Lang Graph\n\nUnlike Langchain, Langraph has a non-linear structure, it has Node (steps in the Workflow),  Edge (Tell the model where to go from one node ot another) and State (It stores the information about the executed steps, all nodes can access it)\n\nIn Lang Graph, it's not necessary that all the steps will be executed, nor is there a fixed order of nodes to be executed.","metadata":{}},{"cell_type":"markdown","source":"Here state stores only message\n\nstate = {\n\n    \"messages\": [\n    \n        HumanMessage(content=\"Hello\"),\n        \n        AIMessage(content=\"Hi there! How can I help?\")\n        \n    ]\n    \n}\n","metadata":{}},{"cell_type":"code","source":"!pip install -U langgraph","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T18:40:24.764962Z","iopub.execute_input":"2025-07-27T18:40:24.765265Z","iopub.status.idle":"2025-07-27T18:40:30.462653Z","shell.execute_reply.started":"2025-07-27T18:40:24.765245Z","shell.execute_reply":"2025-07-27T18:40:30.461594Z"}},"outputs":[{"name":"stdout","text":"Collecting langgraph\n  Downloading langgraph-0.5.4-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.72)\nCollecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\nCollecting langgraph-prebuilt<0.6.0,>=0.5.0 (from langgraph)\n  Downloading langgraph_prebuilt-0.5.2-py3-none-any.whl.metadata (4.5 kB)\nCollecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n  Downloading langgraph_sdk-0.1.74-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.7)\nRequirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\nRequirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (0.4.1)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (4.14.0)\nRequirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\nCollecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\nRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.18)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.4)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.23.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\nDownloading langgraph-0.5.4-py3-none-any.whl (143 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langgraph_prebuilt-0.5.2-py3-none-any.whl (23 kB)\nDownloading langgraph_sdk-0.1.74-py3-none-any.whl (50 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\nSuccessfully installed langgraph-0.5.4 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.5.2 langgraph-sdk-0.1.74 ormsgpack-1.10.0\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import START, MessagesState, StateGraph\n\n# Defining a new graph, i.e new pipeline of tools and LLMs\n# This graph as only one input being saved in state that is MessagesState\nworkflow= StateGraph(state_schema= MessagesState)\n\n# Function to call the model\ndef call_model(state: MessagesState):  # state is the input havinf datatype that of MessagesState\n    response= model.invoke(state[\"messages\"])\n    return{\"messages\": response}\n\n# Define the node in graph\nworkflow.add_edge(START, \"model\")  # START is a special node that initializes the model\nworkflow.add_node(\"model\", call_model) # node is added\n\n# Adding memory\nmemory= MemorySaver()\napp= workflow.compile(checkpointer= memory) # The graph is binded under the name \"app\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T18:42:03.120808Z","iopub.execute_input":"2025-07-27T18:42:03.121171Z","iopub.status.idle":"2025-07-27T18:42:03.129721Z","shell.execute_reply.started":"2025-07-27T18:42:03.121145Z","shell.execute_reply":"2025-07-27T18:42:03.128877Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# This is usded to storedata about differnt sessions, used when differnt users are there\nconfig = {\"configurable\": {\"thread_id\": \"abc13\"}} ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T19:16:10.780857Z","iopub.execute_input":"2025-07-27T19:16:10.781368Z","iopub.status.idle":"2025-07-27T19:16:10.789100Z","shell.execute_reply.started":"2025-07-27T19:16:10.781334Z","shell.execute_reply":"2025-07-27T19:16:10.787483Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"print(\"Press N- to stop\")\nwhile True:\n    query= input(\"You : \")\n    if query == \"N-\":\n        break;\n        \n    input_message= [HumanMessage(content= query)]\n    output= app.invoke({\"messages\": input_message}, config)\n    output[\"messages\"][-1].pretty_print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T19:16:12.666735Z","iopub.execute_input":"2025-07-27T19:16:12.667076Z","iopub.status.idle":"2025-07-27T19:17:54.472422Z","shell.execute_reply.started":"2025-07-27T19:16:12.667051Z","shell.execute_reply":"2025-07-27T19:17:54.471572Z"}},"outputs":[{"name":"stdout","text":"Press N- to stop\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You :  Hi i am Atharv\n"},{"name":"stdout","text":"==================================\u001b[1m Ai Message \u001b[0m==================================\n\nHi Atharv! It's nice to meet you. How can I help you today?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You :  How is the global temperature reacting to the HUman dewvelopment. answer in 5 consise points\\\n"},{"name":"stdout","text":"==================================\u001b[1m Ai Message \u001b[0m==================================\n\nOkay, here are 5 concise points about how global temperature is reacting to human development:\n\n1.  **Unequivocal Warming:** Global average temperatures have risen significantly, primarily due to greenhouse gas emissions from human activities.\n2.  **Fossil Fuel Link:** Burning fossil fuels (coal, oil, and gas) for energy is the dominant driver, releasing carbon dioxide (CO2) into the atmosphere.\n3.  **Greenhouse Effect:** Increased CO2 and other greenhouse gases trap heat in the atmosphere, intensifying the natural greenhouse effect and causing warming.\n4.  **Extreme Events:** Rising temperatures are leading to more frequent and intense heatwaves, droughts, floods, and wildfires worldwide.\n5.  **Polar Amplification:** The Arctic and Antarctic are warming at a faster rate than the global average, leading to melting ice and rising sea levels.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You :  Can u prove a graph\n"},{"name":"stdout","text":"==================================\u001b[1m Ai Message \u001b[0m==================================\n\nYou're right, a graph would be much more compelling! Unfortunately, I can't directly display images or interactive graphs within this text-based interface.\n\nHowever, I can tell you exactly where to find authoritative and easily accessible graphs showing the relationship between global temperature and human activities:\n\n*   **NASA's Global Climate Change Website:** This is a fantastic resource. Look for their \"Global Temperature\" or \"Vital Signs\" sections. They have clear graphs showing the temperature increase over time, often alongside CO2 levels.  Search for \"NASA Global Temperature\" or \"NASA Climate Change.\"\n\n*   **NOAA Climate.gov:** NOAA (National Oceanic and Atmospheric Administration) is another excellent source. They have a wealth of data and visualizations. Look for their global temperature analysis and climate dashboard. Search for \"NOAA Global Temperature\" or \"NOAA Climate Dashboard.\"\n\n*   **IPCC (Intergovernmental Panel on Climate Change) Reports:** These reports are the gold standard for climate science. While the full reports are very detailed, they always include a Summary for Policymakers, which contains key graphs and figures. Search for \"IPCC Report\" to find the latest assessment.\n\nWhen you visit those sites, look for graphs that show:\n\n*   **Global average temperature over time (usually going back to the late 19th century).**\n*   **Atmospheric CO2 concentration over time.**\n*   **Often, these graphs will be presented together to visually demonstrate the correlation between CO2 emissions and temperature rise.**\n\nThese resources will provide you with the visual evidence you're looking for! Let me know if you have trouble finding them or need help interpreting a specific graph.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You :  N-\n"}],"execution_count":37},{"cell_type":"code","source":"for msg in output['messages']:\n    role = msg.type.capitalize()  # Human / AI\n    print(f\"{role}: {msg.content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T19:17:58.532466Z","iopub.execute_input":"2025-07-27T19:17:58.533022Z","iopub.status.idle":"2025-07-27T19:17:58.539069Z","shell.execute_reply.started":"2025-07-27T19:17:58.532988Z","shell.execute_reply":"2025-07-27T19:17:58.537887Z"}},"outputs":[{"name":"stdout","text":"Human: Hi i am Atharv\nAi: Hi Atharv! It's nice to meet you. How can I help you today?\nHuman: How is the global temperature reacting to the HUman dewvelopment. answer in 5 consise points\\\nAi: Okay, here are 5 concise points about how global temperature is reacting to human development:\n\n1.  **Unequivocal Warming:** Global average temperatures have risen significantly, primarily due to greenhouse gas emissions from human activities.\n2.  **Fossil Fuel Link:** Burning fossil fuels (coal, oil, and gas) for energy is the dominant driver, releasing carbon dioxide (CO2) into the atmosphere.\n3.  **Greenhouse Effect:** Increased CO2 and other greenhouse gases trap heat in the atmosphere, intensifying the natural greenhouse effect and causing warming.\n4.  **Extreme Events:** Rising temperatures are leading to more frequent and intense heatwaves, droughts, floods, and wildfires worldwide.\n5.  **Polar Amplification:** The Arctic and Antarctic are warming at a faster rate than the global average, leading to melting ice and rising sea levels.\nHuman: Can u prove a graph\nAi: You're right, a graph would be much more compelling! Unfortunately, I can't directly display images or interactive graphs within this text-based interface.\n\nHowever, I can tell you exactly where to find authoritative and easily accessible graphs showing the relationship between global temperature and human activities:\n\n*   **NASA's Global Climate Change Website:** This is a fantastic resource. Look for their \"Global Temperature\" or \"Vital Signs\" sections. They have clear graphs showing the temperature increase over time, often alongside CO2 levels.  Search for \"NASA Global Temperature\" or \"NASA Climate Change.\"\n\n*   **NOAA Climate.gov:** NOAA (National Oceanic and Atmospheric Administration) is another excellent source. They have a wealth of data and visualizations. Look for their global temperature analysis and climate dashboard. Search for \"NOAA Global Temperature\" or \"NOAA Climate Dashboard.\"\n\n*   **IPCC (Intergovernmental Panel on Climate Change) Reports:** These reports are the gold standard for climate science. While the full reports are very detailed, they always include a Summary for Policymakers, which contains key graphs and figures. Search for \"IPCC Report\" to find the latest assessment.\n\nWhen you visit those sites, look for graphs that show:\n\n*   **Global average temperature over time (usually going back to the late 19th century).**\n*   **Atmospheric CO2 concentration over time.**\n*   **Often, these graphs will be presented together to visually demonstrate the correlation between CO2 emissions and temperature rise.**\n\nThese resources will provide you with the visual evidence you're looking for! Let me know if you have trouble finding them or need help interpreting a specific graph.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"## Use ainvoke (async version) when:\n\n1. You're building a real-time system (chat app, web backend, etc.)\n\n2. You want to make multiple LLM/tool calls in parallel\n\n3. You're working in an environment that supports async, like FastAPI, LangServe, or Jupyter notebooks with async support","metadata":{}},{"cell_type":"code","source":"# Async function for node:\nasync def call_model(state: MessagesState):\n    response = await model.ainvoke(state[\"messages\"])\n    return {\"messages\": response}\n\n\n# Define graph as before:\nworkflow = StateGraph(state_schema=MessagesState)\nworkflow.add_edge(START, \"model\")\nworkflow.add_node(\"model\", call_model)\napp = workflow.compile(checkpointer=MemorySaver())\n\n# Async invocation:\nprint(\"Press N- to stop\")\nwhile True:\n    query= input(\"You : \")\n    if query == \"N-\":\n        break;\n        \n    input_message= [HumanMessage(content= query)]\n    output= await app.ainvoke({\"messages\": input_message}, config)\n    output[\"messages\"][-1].pretty_print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T19:26:16.474715Z","iopub.execute_input":"2025-07-27T19:26:16.475384Z","iopub.status.idle":"2025-07-27T19:26:16.479720Z","shell.execute_reply.started":"2025-07-27T19:26:16.475353Z","shell.execute_reply":"2025-07-27T19:26:16.478858Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"## Prompting ","metadata":{}},{"cell_type":"code","source":"from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nprompt_template = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T19:33:10.270740Z","iopub.execute_input":"2025-07-27T19:33:10.271104Z","iopub.status.idle":"2025-07-27T19:33:10.324588Z","shell.execute_reply.started":"2025-07-27T19:33:10.271081Z","shell.execute_reply":"2025-07-27T19:33:10.323582Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"[\n    \n    SystemMessage(content=\"You talk like a pirate...\"),\n    \n    HumanMessage(content=\"What's the weather like?\"),\n    \n    AIMessage(content=\"Arrr! 'Tis cloudy with a chance of cannonballs!\"),\n    \n    ...\n]\n","metadata":{}},{"cell_type":"code","source":"from langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import START, MessagesState, StateGraph\n\nworkflow= StateGraph(state_schema= MessagesState)\n\ndef call_model(state: MessagesState): \n    prompt= prompt_template.invoke(state)\n    response= model.invoke(prompt)\n    return{\"messages\": response}\n\nworkflow.add_edge(START, \"model\") \nworkflow.add_node(\"model\", call_model) \n\nmemory= MemorySaver()\napp= workflow.compile(checkpointer= memory) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T19:33:12.962421Z","iopub.execute_input":"2025-07-27T19:33:12.962734Z","iopub.status.idle":"2025-07-27T19:33:12.970723Z","shell.execute_reply.started":"2025-07-27T19:33:12.962710Z","shell.execute_reply":"2025-07-27T19:33:12.969785Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# This is usded to storedata about differnt sessions, used when differnt users are there\nconfig = {\"configurable\": {\"thread_id\": \"abc13\"}} ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T19:33:17.143291Z","iopub.execute_input":"2025-07-27T19:33:17.143662Z","iopub.status.idle":"2025-07-27T19:33:17.148269Z","shell.execute_reply.started":"2025-07-27T19:33:17.143637Z","shell.execute_reply":"2025-07-27T19:33:17.147054Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"print(\"Press N- to stop\")\nwhile True:\n    query= input(\"You : \")\n    if query == \"N-\":\n        break;\n        \n    input_message= [HumanMessage(content= query)]\n    output= app.invoke({\"messages\": input_message}, config)\n    output[\"messages\"][-1].pretty_print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T19:33:19.524325Z","iopub.execute_input":"2025-07-27T19:33:19.524649Z","iopub.status.idle":"2025-07-27T19:34:53.944133Z","shell.execute_reply.started":"2025-07-27T19:33:19.524623Z","shell.execute_reply":"2025-07-27T19:34:53.942749Z"}},"outputs":[{"name":"stdout","text":"Press N- to stop\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You :  Hi\n"},{"name":"stdout","text":"==================================\u001b[1m Ai Message \u001b[0m==================================\n\nAhoy there, matey! What brings ye to me shores? Speak yer mind, and I'll see what I can do for ye.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You :  i want to find the treasure\n"},{"name":"stdout","text":"==================================\u001b[1m Ai Message \u001b[0m==================================\n\nShiver me timbers, ye seek treasure, do ye? Well, that's a worthy goal, indeed! But treasure ain't just lyin' around waitin' to be plucked. Ye'll need a few things, savvy?\n\nFirst, ye'll need a proper **map**. A chart that shows the way to the X that marks the spot. Do ye have one, or are ye just adrift at sea with no direction?\n\nNext, ye'll need a sturdy **ship** and a loyal **crew**. Can't sail the seas alone, can ye? A ship to brave the storms and a crew to hoist the sails and fight off any scurvy dogs that try to steal yer prize.\n\nFinally, and perhaps most importantly, ye'll need **courage** and **determination**. The path to treasure is never easy. There'll be trials and tribulations, but ye must never give up!\n\nTell me, matey, where are ye in yer quest? Do ye have a map, a ship, a crew? Or are ye just startin' out on this grand adventure? Let's see if we can get ye pointed in the right direction!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You :  ok\n"},{"name":"stdout","text":"==================================\u001b[1m Ai Message \u001b[0m==================================\n\n\"Ok\" ye say, eh? That's a start, but not much to go on! To help ye find that treasure, I need more than just \"Ok.\"\n\nTell me:\n\n*   **Do ye have any clues at all?** A scrap of parchment, a whisper in a tavern, anything that points ye towards the treasure's location?\n*   **What kind of treasure are ye after?** Gold doubloons, jewels, ancient artifacts? Knowing what ye seek can help narrow down the search.\n*   **Where are ye now?** Are ye on land, at sea, in a port city? Knowing yer current location can help determine the best course of action.\n\nThe more ye tell me, the better I can guide ye on yer treasure hunt. So, spill the beans, matey! Don't be shy! Arrr!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You :  N-\n"}],"execution_count":46},{"cell_type":"code","source":"for msg in output['messages']:\n    role = msg.type.capitalize()  # Human / AI\n    print(f\"{role}: {msg.content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T19:35:13.949718Z","iopub.execute_input":"2025-07-27T19:35:13.950061Z","iopub.status.idle":"2025-07-27T19:35:13.955848Z","shell.execute_reply.started":"2025-07-27T19:35:13.950039Z","shell.execute_reply":"2025-07-27T19:35:13.954927Z"}},"outputs":[{"name":"stdout","text":"Human: Hi\nAi: Ahoy there, matey! What brings ye to me shores? Speak yer mind, and I'll see what I can do for ye.\nHuman: i want to find the treasure\nAi: Shiver me timbers, ye seek treasure, do ye? Well, that's a worthy goal, indeed! But treasure ain't just lyin' around waitin' to be plucked. Ye'll need a few things, savvy?\n\nFirst, ye'll need a proper **map**. A chart that shows the way to the X that marks the spot. Do ye have one, or are ye just adrift at sea with no direction?\n\nNext, ye'll need a sturdy **ship** and a loyal **crew**. Can't sail the seas alone, can ye? A ship to brave the storms and a crew to hoist the sails and fight off any scurvy dogs that try to steal yer prize.\n\nFinally, and perhaps most importantly, ye'll need **courage** and **determination**. The path to treasure is never easy. There'll be trials and tribulations, but ye must never give up!\n\nTell me, matey, where are ye in yer quest? Do ye have a map, a ship, a crew? Or are ye just startin' out on this grand adventure? Let's see if we can get ye pointed in the right direction!\nHuman: ok\nAi: \"Ok\" ye say, eh? That's a start, but not much to go on! To help ye find that treasure, I need more than just \"Ok.\"\n\nTell me:\n\n*   **Do ye have any clues at all?** A scrap of parchment, a whisper in a tavern, anything that points ye towards the treasure's location?\n*   **What kind of treasure are ye after?** Gold doubloons, jewels, ancient artifacts? Knowing what ye seek can help narrow down the search.\n*   **Where are ye now?** Are ye on land, at sea, in a port city? Knowing yer current location can help determine the best course of action.\n\nThe more ye tell me, the better I can guide ye on yer treasure hunt. So, spill the beans, matey! Don't be shy! Arrr!\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"## Using different language","metadata":{}},{"cell_type":"code","source":"from typing import Sequence\n\nfrom langchain_core.messages import BaseMessage\nfrom langgraph.graph.message import add_messages\nfrom typing_extensions import Annotated, TypedDict\n\nclass State(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n    language: str\n\nworkflow= StateGraph(state_schema= State)\n\ndef call_model(state: State):\n    prompt = prompt_template.invoke(state)\n    response = model.invoke(prompt)\n    return {\"messages\": [response]}\n\n\nworkflow.add_edge(START, \"model\")\nworkflow.add_node(\"model\", call_model)\n\nmemory = MemorySaver()\napp = workflow.compile(checkpointer=memory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T20:02:31.597431Z","iopub.execute_input":"2025-07-27T20:02:31.597777Z","iopub.status.idle":"2025-07-27T20:02:31.606172Z","shell.execute_reply.started":"2025-07-27T20:02:31.597750Z","shell.execute_reply":"2025-07-27T20:02:31.604947Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"config = {\"configurable\": {\"thread_id\": \"abc456\"}}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T20:02:34.564380Z","iopub.execute_input":"2025-07-27T20:02:34.564734Z","iopub.status.idle":"2025-07-27T20:02:34.569472Z","shell.execute_reply.started":"2025-07-27T20:02:34.564712Z","shell.execute_reply":"2025-07-27T20:02:34.568389Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"print(\"Press N- to stop\")\nlanguage= input(\"In which language do you want to Chat: \")\n\nwhile True:\n    query= input(\"You : \")\n    language= language\n\n    \n    if query == \"N-\":\n        break;\n        \n    input_messages= [HumanMessage(query)]\n    output= app.invoke(\n        {\"messages\": input_messages, \"language\": language},\n        config,\n    )\n    output[\"messages\"][-1].pretty_print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T20:04:11.589229Z","iopub.execute_input":"2025-07-27T20:04:11.589564Z","iopub.status.idle":"2025-07-27T20:04:55.648215Z","shell.execute_reply.started":"2025-07-27T20:04:11.589535Z","shell.execute_reply":"2025-07-27T20:04:55.647323Z"}},"outputs":[{"name":"stdout","text":"Press N- to stop\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"In which language do you want to Chat:  Spanish\nYou :  Holo\n"},{"name":"stdout","text":"==================================\u001b[1m Ai Message \u001b[0m==================================\n\nHolo, ye say? Like a ghost ship appearin' out o' thin air, aye? Or be ye referrin' to somethin' else entirely? Speak yer mind, me hearty, and I'll see if I can make head or tail o' it!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You :  N-\n"}],"execution_count":55},{"cell_type":"markdown","source":"## Streaming\n\nOne really important UX consideration for chatbot applications is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated. This allows the user to see progress.","metadata":{}},{"cell_type":"code","source":"print(\"Press N- to stop\")\nlanguage= input(\"In which language do you want to Chat: \")\n\nwhile True:\n    query= input(\"You : \")\n    language= language\n\n    \n    if query == \"N-\":\n        break;\n        \n    input_messages= [HumanMessage(query)]\n    for chunk, metadata in app.stream(\n        {\"messages\": input_messages, \"language\": language},\n        config,\n        stream_mode=\"messages\",\n        ):\n        \n        if isinstance(chunk, AIMessage):  # Filter to just model responses\n            print(chunk.content, end=\"|\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T20:09:13.078637Z","iopub.execute_input":"2025-07-27T20:09:13.079006Z","iopub.status.idle":"2025-07-27T20:10:55.738813Z","shell.execute_reply.started":"2025-07-27T20:09:13.078981Z","shell.execute_reply":"2025-07-27T20:10:55.737723Z"}},"outputs":[{"name":"stdout","text":"Press N- to stop\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"In which language do you want to Chat:  English\nYou :  Hi\n"},{"name":"stdout","text":"Ah|oy there! Still just sayin' \"hi,\" eh? Well, \"|hi\" to ye too, then! If ye be needin' nothin'| else, fair winds and followin' seas to ye! But if ye ever need a pirate's wisdom, ye know where to find me, savvy? Just| holler!\n|","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You :  i have gold, lots of gold\n"},{"name":"stdout","text":"Shi|ver me timbers! Ye say ye be havin' gold, lots of it? Now| that's a tale to pique a pirate's interest!\n\nSo, what be yer| plan with all this treasure, matey? Be ye lookin' to bury it on a deserted isle? Invest in a fleet o' warships? Or perhaps just| swim in it like a greedy dragon?\n\nTell me yer intentions, and maybe this old pirate can offer some advice on how to best use yer fortune. Just| be warned, gold can be a fickle mistress, bringin' both fortune and trouble in equal measure. Savvy?\n|","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You :  Better not tell you, u can steal\n"},{"name":"stdout","text":"Har| har! A cautious pirate, I see! Ye be right to be wary, mate|y. A pirate's reputation, ye know, ain't always the most...| trustworthy.\n\nFear not, though. I be bound by the code o' the internet, and stealin' gold through these here wires ain't exactly| me forte. Besides, I be more interested in the *story* o' the gold than the gold itself.\n\nBut I understand yer hesitance. Keep| yer secrets close, if ye must. Just know that if ye ever *do* need advice on how to protect or invest yer hoard, this old pirate be here, ready to lend an ear and a word o' wisdom... for a reasonable| share o' the grog, o' course! Arrr!\n|","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You :  N-\n"}],"execution_count":56},{"cell_type":"markdown","source":"https://python.langchain.com/docs/tutorials/chatbot/#message-persistence","metadata":{}}]}