{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RAG ~ Retrieval Augmented Generation\n\nBasically an LLM + Retriever, It saves us the time of fine tuning. \n\nFine-tuning v/s RAG:\n\nIn fine tuning we re-train the model on new data and it's weights are updated, it changes the way the model would give out the results. In a RAG we do not retrain the model instead the model fetches information from provided data and gives output on the basis of retrieved data.\n\nThe LLM in a RAG just takes input of retrieved data and synthesize the output, so it is only the retrival part ehich is new. ","metadata":{}},{"cell_type":"markdown","source":"## Retrival\n\nThe newly given information (text, doc or etc) is stored in a database (Vector DB, Graph DB or traditional SQL). \n\nVector DB: The data is divide into chunks and then embedded and then stored in vector DB, then query is also vectorized and we find the vector closest to the query and we select top K matches and send to the LLM for synthesis. It may sometimes fetch irrelevent data.\n\nGraph DB: The data is broken into entities and it is stored as a graph with connetions b\\w entites capturing their relationships, the query is also converted into a graph and the exact matches are searched for and then LLM synthesizes the answer. As  it requires exact matching it maybe irrelevent for some applications.\n\nSQL: Traditional way to store data, then it uses TF-IDF for retriving data but we lose semantic flexibility.\n\n","metadata":{}},{"cell_type":"markdown","source":"Let’s look at two different types of retrieval methods: Standard, Sentence window and auto-merging.\n\nStandard: The Chunk size is same for both retrieval and synthesis, it makes the data uniform for both the processes but it maybe problematic because synthesis part may require longer chunk for / more info to produce answer.\n\nSentence window and auto-merging: In this the text is broken units like senetences or group sentences and uses smtg like +- 2 sentences from target sentence. This method does not return only the exact matched sentence instead, it returns a window of surrounding context for synthesis.\n\n\nWhat is Retriever Ensembling?\nIdea: Instead of sticking to one way of chunking your documents or using a single retriever strategy, you try multiple at once.\n\nChunk size matters: Different chunk sizes capture different amounts of context. Small chunks may be precise but lack context; large chunks have more context but might introduce noise.\n\nThe process is as follows:\n1. Chunk up the same document in a bunch of different ways, say with chunk sizes: 128, 256, 512, and 1024.\n2. During retrieval, we fetch relevant chunks from each retriever, thus ensembling them together for retrieval.\n3. Use a re-ranker to rank results accoring to their relevance to query.\n\n*Note: A re-ranker is a component used after retrieval in a RAG system to re-evaluate and reorder the initial set of retrieved documents or chunks based on how relevant they are to the query.*\n\n1. Lexical Re-ranking:\nBased on exact word matching.\nExample techniques: BM25, TF-IDF cosine similarity.\n\n2. Semantic Re-ranking:\nUses transformer-based models (e.g., BERT, DistilBERT) to understand semantic meaning, not just word overlap.\nThe model is asked: “Which of these chunks best answers this question?”\n\n3. Learning to Rank (LTR):\nYou train a model specifically for ranking documents.\n    Three types:\n    Point-wise: Score each document independently.\n    Pair-wise: Compare pairs of documents to see which is better.\n    List-wise: Consider the whole list at once and reorder.\n\n","metadata":{}},{"cell_type":"code","source":"%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:14:36.692704Z","iopub.execute_input":"2025-07-31T10:14:36.692913Z","iopub.status.idle":"2025-07-31T10:14:50.658052Z","shell.execute_reply.started":"2025-07-31T10:14:36.692892Z","shell.execute_reply":"2025-07-31T10:14:50.656877Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m527.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.2/152.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.8/442.8 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# LangSmith\n\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.","metadata":{}},{"cell_type":"markdown","source":"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","metadata":{}},{"cell_type":"code","source":"import getpass\nimport os\n\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:14:50.659345Z","iopub.execute_input":"2025-07-31T10:14:50.659601Z","iopub.status.idle":"2025-07-31T10:16:51.189385Z","shell.execute_reply.started":"2025-07-31T10:14:50.659573Z","shell.execute_reply":"2025-07-31T10:16:51.188510Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":" ········\n"}],"execution_count":2},{"cell_type":"markdown","source":"## Chat Model - Google Gemini","metadata":{}},{"cell_type":"code","source":"pip install -qU \"langchain[google-genai]\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:16:51.192122Z","iopub.execute_input":"2025-07-31T10:16:51.192401Z","iopub.status.idle":"2025-07-31T10:17:01.212716Z","shell.execute_reply.started":"2025-07-31T10:16:51.192380Z","shell.execute_reply":"2025-07-31T10:17:01.211412Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import getpass\nimport os\n\nif not os.environ.get(\"GOOGLE_API_KEY\"):\n  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n\nfrom langchain.chat_models import init_chat_model\n\nllm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:17:01.214042Z","iopub.execute_input":"2025-07-31T10:17:01.214312Z","iopub.status.idle":"2025-07-31T10:17:12.936165Z","shell.execute_reply.started":"2025-07-31T10:17:01.214282Z","shell.execute_reply":"2025-07-31T10:17:12.935110Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter API key for Google Gemini:  ········\n"}],"execution_count":4},{"cell_type":"markdown","source":"## Embedding - Google Gemini","metadata":{}},{"cell_type":"code","source":"import getpass\nimport os\n\nif not os.environ.get(\"GOOGLE_API_KEY\"):\n  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\n\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:17:12.937190Z","iopub.execute_input":"2025-07-31T10:17:12.937671Z","iopub.status.idle":"2025-07-31T10:17:12.947527Z","shell.execute_reply.started":"2025-07-31T10:17:12.937641Z","shell.execute_reply":"2025-07-31T10:17:12.946620Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Vector DB - Chroma","metadata":{}},{"cell_type":"code","source":"pip install -qU langchain-chroma","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:17:12.948355Z","iopub.execute_input":"2025-07-31T10:17:12.948864Z","iopub.status.idle":"2025-07-31T10:17:51.020859Z","shell.execute_reply.started":"2025-07-31T10:17:12.948833Z","shell.execute_reply":"2025-07-31T10:17:51.019343Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1753957032.988332      36 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.1/321.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from langchain_chroma import Chroma\n\nvector_store = Chroma(\n    collection_name=\"example_collection\",\n    embedding_function=embeddings,\n    # persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T10:17:51.022637Z","iopub.execute_input":"2025-07-31T10:17:51.023062Z","iopub.status.idle":"2025-07-31T10:17:51.817973Z","shell.execute_reply.started":"2025-07-31T10:17:51.023020Z","shell.execute_reply":"2025-07-31T10:17:51.817150Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Defining the Agent\n","metadata":{}},{"cell_type":"markdown","source":"1. Make a Customised state function: Question(from user), Context(from the given info), Answer(from LLM)\n2. Make tool for retrival, generation","metadata":{}},{"cell_type":"code","source":"# Libraries to scrape data fromm a website\nimport bs4\nfrom typing import Literal\nfrom langchain import hub\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_core.documents import Document\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_core.vectorstores import InMemoryVectorStore\n\n# Libraries for making the agent graph\nfrom langgraph.graph import START, StateGraph\nfrom typing_extensions import List, TypedDict, Annotated\n\n# Download and extract relevant content from a blog post.\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n    ),\n)\ndocs = loader.load()\n\n# Split the loaded text into manageable, overlapping chunks.\ntext_splitter= RecursiveCharacterTextSplitter(chunk_size= 1000, chunk_overlap= 250)\nall_splits= text_splitter.split_documents(docs)\n\n# Update metadata (illustration purposes)\ntotal_documents = len(all_splits)\nthird = total_documents // 3\n\nfor i, document in enumerate(all_splits):\n    if i < third:\n        document.metadata[\"section\"] = \"beginning\"\n    elif i < 2 * third:\n        document.metadata[\"section\"] = \"middle\"\n    else:\n        document.metadata[\"section\"] = \"end\"\n\n# Store the document chunks in a vector store \n# an embedding-based database for fast semantic search\nvector_store = InMemoryVectorStore(embeddings)\n_ = vector_store.add_documents(documents= all_splits)\n\n# Define scheme for Search\nclass Search(TypedDict):\n    \"\"\"Search query.\"\"\"\n\n    query: Annotated[str, ..., \"Search query to run.\"]\n    section: Annotated[\n        Literal[\"beginning\", \"middle\", \"end\"],\n        ...,\n        \"Section to query.\",\n    ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T11:53:47.542537Z","iopub.execute_input":"2025-07-31T11:53:47.542850Z","iopub.status.idle":"2025-07-31T11:53:49.700891Z","shell.execute_reply.started":"2025-07-31T11:53:47.542827Z","shell.execute_reply":"2025-07-31T11:53:49.700105Z"}},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"prompt for answering for the chatbot","metadata":{}},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\n\n# Define custom prompt template as a string\ntemplate= \"\"\"\nYou are an William Shakesphere. Answer the question based on given context\n\nContext:\n{context}\n\nQuestion:\n{question}\n\nAnswer:\n\"\"\"\n\n# Create a PromptTemplate object\nprompt= PromptTemplate(\n    input_variables= [\"context\", \"question\"],\n    template= template,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T11:55:48.296203Z","iopub.execute_input":"2025-07-31T11:55:48.296513Z","iopub.status.idle":"2025-07-31T11:55:48.302418Z","shell.execute_reply.started":"2025-07-31T11:55:48.296491Z","shell.execute_reply":"2025-07-31T11:55:48.301387Z"}},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":"The Agent will not remember your convo history","metadata":{}},{"cell_type":"code","source":"# Define state\n\nclass State(TypedDict):\n    question: str\n    query: Search\n    context: List[Document]\n    answer: str\n\ndef analyze_query(state: State):\n    structured_llm= llm.with_structured_output(Search)\n    query = structured_llm.invoke(state[\"question\"])\n    return {\"query\": query}\n    \n# Define retriever\ndef retrieve(state: State):\n    query= state[\"query\"]\n    retrieved_docs= vector_store.similarity_search(\n        query[\"query\"],\n        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],)\n    return {\"context\": retrieved_docs}\n\n# Define generator\ndef generate(state: State):\n    docs_content= \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n    messages= prompt.format(context= docs_content, question= state[\"question\"])\n    response= llm.invoke(messages)\n    return {\"answer\": response.content}\n\ngraph= StateGraph(State).add_sequence([analyze_query, retrieve, generate])\ngraph.add_edge(START, \"analyze_query\")\napp= graph.compile()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T11:55:51.360826Z","iopub.execute_input":"2025-07-31T11:55:51.361186Z","iopub.status.idle":"2025-07-31T11:55:51.373646Z","shell.execute_reply.started":"2025-07-31T11:55:51.361161Z","shell.execute_reply":"2025-07-31T11:55:51.372516Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"response= app.invoke({\"question\": \"What is the article about?  Explain points\"})\nprint(response[\"answer\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T11:55:54.905455Z","iopub.execute_input":"2025-07-31T11:55:54.905776Z","iopub.status.idle":"2025-07-31T11:56:00.827848Z","shell.execute_reply.started":"2025-07-31T11:55:54.905752Z","shell.execute_reply":"2025-07-31T11:56:00.826908Z"}},"outputs":[{"name":"stdout","text":"Hark, gentle audience, lend thine ears! This scroll, though writ in the dry language of scholars, doth speak of memory, both human and artificial, and how the one informs the other.\n\n**Firstly, 'tis about Maximum Inner Product Search, a curious beast!** Imagine searching through a vast library of knowledge, not by title, but by how closely related each book is to a question in your mind. That, in essence, is MIPS. It allows a swift sifting through a sea of information, returning the closest matches, though perhaps not with perfect accuracy, for the sake of haste.\n\n**Secondly, the author doth liken this process to the workings of the human mind!** He speaks of Short-Term Memory, a fleeting stage where thoughts reside but briefly, like actors upon a stage, before fading into the wings. Then, Long-Term Memory, a vast storehouse where knowledge is kept for years, like ancient tomes upon dusty shelves.\n\n*   **Sensory memory** the learning embedding representations for raw inputs, including text, image or other modalities;\n*   **Short-term memory** as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\n*   **Long-term memory** as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\n\n**Thirdly, the author doth suggest a parallel!** He maps these human memories to the workings of machines. Sensory memory is akin to the embedding representations, Short-Term Memory becomes in-context learning, constrained by the limits of the machine's \"attention span,\" and Long-Term Memory is likened to an external database, a vast store accessed through swift, albeit imperfect, retrieval methods.\n\n**Lastly, the author touches upon the training of these artificial minds!** He speaks of methods to prevent \"overfitting\" and \"shortcutting,\" lest the machine merely copy what it has seen, rather than truly understanding. A combination of datasets are used to train the \"mind\"\n\nThus, the article doth explore the interplay between human memory and artificial intelligence, suggesting that by understanding how we remember, we may better design machines that can learn and recall with greater efficiency. A fascinating subject, indeed!\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}